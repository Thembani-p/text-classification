{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "The kernel is restarted every time the parameters are changed to make reproducibility easier. \n",
    "\n",
    "#### The Pretrained Model\n",
    "\n",
    "The first few blocks format the data and create the training and validation split. The blocks up until the section on hyper parameters must be run. Then under the 'loading the model' section the last block should be run. This block is entitled load model. In the 'Final model' section the block, entitled 'accuracy rates' can be run. This will run the model on the preprocessed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look into preprocessor.py file for details\n",
    "from preprocessor import DocumentTermMatrix, accuracy, indicator_to_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_data = DocumentTermMatrix(\"yelp_academic_dataset_review.json\", \"text\", \"stars\", 1000) #  TDM with IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_classes = len(yelp_data.docs_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "# data split - training and validation data (80, 20)\n",
    "X_train, X_test,Y_train, Y_test = cross_validation.train_test_split(yelp_data.X_docs,yelp_data.Y_docs,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter \n",
    "\n",
    "The code in this section is specifically for setting hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Building model...', 0)\n",
      "('Building model...', 1)\n",
      "('Building model...', 2)\n",
      "('Building model...', 3)\n"
     ]
    }
   ],
   "source": [
    "models = ['' for i in xrange(4)] \n",
    "for cv in xrange(4):\n",
    "    \n",
    "    print('Building model...', cv)\n",
    "    models[cv] = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessor import Kfold_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_index = [i for i in xrange(len(X_train))]\n",
    " \n",
    "indices_cv = Kfold_cv(full_index,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histories = []\n",
    "for cv in xrange(4):\n",
    "    np.random.seed(1337)  # for reproducibility\n",
    "    # data split\n",
    "    #  this could be done in a single shot before hand\n",
    "    #  at the moment this will preserve memory which seems import in a VM setting\n",
    "    x_train = X_train[indices_cv[cv][\"train\"]] \n",
    "    x_test  = X_train[indices_cv[cv][\"test\"]] \n",
    "\n",
    "    y_train = [ lang_data.docs_label_index[Y_train[i]] for i in indices_cv[cv][\"train\"] ] \n",
    "    y_test  = [ lang_data.docs_label_index[Y_train[i]] for i in indices_cv[cv][\"test\"] ] \n",
    "\n",
    "    # create appropirate matrix (hot encoded) response\n",
    "    # y_train, y_test = [indicator_to_matrix(x,lang_data.docs_label_index)  for x in (y_train, y_test)]\n",
    "\n",
    "    history = models[cv].fit(x_train, y_train) \n",
    "    \n",
    "    # set validation split to 0 or none so all the traning data is used \n",
    "    #   the out of sample rate will be determined later\n",
    "\n",
    "    # do not set verbose = 1\n",
    "    \n",
    "    histories.append(history) # don't know how kosher this is, seems worth trying though :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in and out scores over the cross folds\n",
    "\n",
    "out_sample_accuracies = []\n",
    "in_sample_accuracies = []\n",
    "confustion_matrices = []\n",
    "for cv in xrange(4):\n",
    "    x_train = lang_data.X_docs[indices_cv[cv][\"train\"]] \n",
    "    x_test  = lang_data.X_docs[indices_cv[cv][\"test\"]] \n",
    "\n",
    "    y_train = [ lang_data.docs_label_index[lang_data.Y_docs[i]] for i in indices_cv[cv][\"train\"] ] \n",
    "    y_test  = [ lang_data.docs_label_index[lang_data.Y_docs[i]] for i in indices_cv[cv][\"test\"] ] \n",
    "\n",
    "    # y_test_vec = [ lang_data.docs_label_index[i] for i in y_test ] \n",
    "\n",
    "    # create appropirate matrix (hot encoded) response\n",
    "    #y_train, y_test = [indicator_to_matrix(x,lang_data.docs_label_index)  for x in (y_train, y_test)]\n",
    "    \n",
    "    train_pred = models[cv].predict(x_train)\n",
    "    test_pred = models[cv].predict(x_test)\n",
    "    \n",
    "    train_acc = 100*float(np.array([ train_pred[idx] == y_train[idx]  for idx, pred in enumerate(train_pred)]).sum())/len(train_pred)\n",
    "    test_acc  = 100*float(np.array([ test_pred[idx]  == y_test[idx]   for idx, pred in enumerate(test_pred )]).sum())/len(test_pred )\n",
    "    \n",
    "    out_sample_accuracies.append(test_acc)\n",
    "    in_sample_accuracies.append(train_acc)\n",
    "    \n",
    "    #confustion_matrices.append(confusion_matrix(y_test,test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[87.907608695652172, 87.273550724637687]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean(in_sample_accuracies),np.mean(out_sample_accuracies)]\n",
    "\n",
    "# [87.907608695652172, 87.273550724637687]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "y_train = [ yelp_data.docs_label_index[i] for i in Y_train ] \n",
    "y_test  = [ yelp_data.docs_label_index[i] for i in Y_test ] \n",
    "\n",
    "history = model.fit(X_train, y_train) \n",
    "    \n",
    "# set validation split to 0 or none so all the traning data is used \n",
    "#   the out of sample rate will be determined later\n",
    "\n",
    "# do not set verbose = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98.25, 38.0]\n"
     ]
    }
   ],
   "source": [
    "# accuracy rates\n",
    "\n",
    "y_train = [ yelp_data.docs_label_index[i] for i in Y_train ] \n",
    "y_test  = [ yelp_data.docs_label_index[i] for i in Y_test ] \n",
    "\n",
    "train_pred = model.predict(X_train)\n",
    "test_pred = model.predict(X_test)\n",
    "    \n",
    "train_acc = 100*float(np.array([ train_pred[idx] == y_train[idx]  for idx, pred in enumerate(train_pred)]).sum())/len(train_pred)\n",
    "test_acc  = 100*float(np.array([ test_pred[idx]  == y_test[idx]   for idx, pred in enumerate(test_pred )]).sum())/len(test_pred )\n",
    "    \n",
    "print([train_acc,test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  8  2  3  0]\n",
      " [ 4 10 10  3  2]\n",
      " [ 0  5  9 15  7]\n",
      " [ 1  5 21 27 15]\n",
      " [ 3  4  2 14 22]]\n"
     ]
    }
   ],
   "source": [
    "# confustion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confustion_matrix = (confusion_matrix(y_test,test_pred))\n",
    "print(confustion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "import cPickle\n",
    "# save the classifier\n",
    "with open('yelp_nb.pkl', 'wb') as fid:\n",
    "    cPickle.dump(model, fid)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "import cPickle\n",
    "with open('yelp_nb.pkl', 'rb') as fid:\n",
    "    model = cPickle.load(fid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow' # set backend\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, GRU\n",
    "from keras.utils import np_utils, generic_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pre-trained vectors\n",
    "import cPickle\n",
    "with open('pickles/embeddings_preprocessed_29_4_64.pkl', 'rb') as fid:\n",
    "    embeddings = cPickle.load(fid)\n",
    "with open('pickles/dictionary_preprocessed_29_4_64.pkl', 'rb') as fid:\n",
    "    dictionary = cPickle.load(fid)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocessor import Tensor_Sequence_W2V, accuracy, indicator_to_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_data = Tensor_Sequence_W2V(\"yelp_academic_dataset_review.json\", \"text\", \"stars\", embeddings, dictionary, 100,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1423"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yelp_data.docs_vocab) # vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 7, 2: 3, 3: 4, 4: 15, 5: 16}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class_balance = {1: 0, 2:0, 3:0, 4:0, 5:0}\n",
    "class_balance = {}\n",
    "\n",
    "for i in yelp_data.Y_doc_seq:\n",
    "    if(i in class_balance): class_balance[i] += 1\n",
    "    else: class_balance[i] = 1\n",
    "class_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_data.X_doc_seq.shape[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameter testing with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_models = []\n",
    "models = ['' for i in xrange(4)] \n",
    "# models are training incrementally \n",
    "#   therefore we need multiple copies of the models \n",
    "#   otherwise we end up training on all the data (test and trianing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = yelp_data.maxlen\n",
    "hidden_dim = 128\n",
    "nb_classes = len(yelp_data.docs_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_data.X_doc_seq.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Building model...\n",
      "Building model...\n",
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "for cv in xrange(4):\n",
    "    \n",
    "    print('Building model...')\n",
    "    current_model = Input(shape=yelp_data.X_doc_seq.shape[1:3], dtype='float32')\n",
    "    \n",
    "    # bidirectional LSTM\n",
    "    forwards  = LSTM(hidden_dim,dropout_W=0.1,dropout_U=0.1)(current_model)\n",
    "    backwards = LSTM(hidden_dim,dropout_W=0.1,dropout_U=0.1,go_backwards=True)(current_model)\n",
    "    \n",
    "    # merge LSTM's\n",
    "    merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "    \n",
    "    # add dropout\n",
    "    after_dp = Dropout(0.1)(merged)\n",
    "    \n",
    "    # output\n",
    "    output    = Dense(nb_classes, activation='softmax')(after_dp)\n",
    "    models[cv] = Model(input=current_model, output=output)\n",
    "    \n",
    "    # compile model with adam\n",
    "    models[cv].compile('adam', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7ff776853490>,\n",
       " <keras.engine.training.Model at 0x7ff75cd84c90>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0:2] # check that models are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)               (None, 30, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                      (None, 128)         98816       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                      (None, 128)         98816       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                    (None, 256)         0           lstm_1[0][0]                     \n",
      "                                                                   lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)                (None, 256)         0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                    (None, 4)           1028        dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 198660\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data split - training and validation data (80, 20)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(yelp_data.X_doc_seq,yelp_data.Y_doc_seq,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessor import Kfold_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_index = [i for i in xrange(len(X_train))]\n",
    "indices_cv = Kfold_cv(full_index,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test  = []\n",
    "\n",
    "y_train = []\n",
    "y_test  = []\n",
    "\n",
    "for cv in xrange(4):\n",
    "    x_train.append(X_train[indices_cv[cv][\"train\"]])\n",
    "    x_test.append(X_train[indices_cv[cv][\"test\"]] )\n",
    "\n",
    "    y_train.append([ Y_train[i] for i in indices_cv[cv][\"train\"] ] )\n",
    "    y_test.append([ Y_train[i] for i in indices_cv[cv][\"test\"] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model specs\n",
    "batch_size = 32\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48.333333333333336, 25.0]\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "out_sample_accuracies = []\n",
    "in_sample_accuracies = []\n",
    "    \n",
    "for cv in xrange(4):\n",
    "    np.random.seed(1337)  # for reproducibility\n",
    "    \n",
    "    # create appropirate matrix (hot encoded) response\n",
    "        \n",
    "    y_train_m, y_test_m = [indicator_to_matrix(x,yelp_data.docs_label_index)  for x in (y_train[cv], y_test[cv])]\n",
    "\n",
    "    history = models[cv].fit(x_train[cv], y_train_m,\n",
    "                        nb_epoch=num_epoch, batch_size=batch_size,\n",
    "                        verbose=False) \n",
    "    \n",
    "    # set validation split to 0 or none so all the traning data is used \n",
    "    #   the out of sample rate will be determined later\n",
    "\n",
    "    # do not set verbose = 1\n",
    "    \n",
    "    out_sample_accuracies.append(accuracy(models[cv],x_test[cv],y_test_m))\n",
    "    in_sample_accuracies.append(accuracy(models[cv],x_train[cv],y_train_m)) \n",
    "    \n",
    "print([np.mean(in_sample_accuracies),np.mean(out_sample_accuracies)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_classes(model,x_test):\n",
    "    predictions = model.predict(x_test)\n",
    "    return [ pred.argmax() for idx, pred in enumerate(predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in and out scores over the cross folds\n",
    "\n",
    "out_sample_accuracies = []\n",
    "in_sample_accuracies = []\n",
    "confustion_matrices = []\n",
    "\n",
    "for cv in xrange(4):\n",
    "        \n",
    "    y_test_vec = [ yelp_data.docs_label_index[i] for i in y_test[cv] ] \n",
    "\n",
    "    # create appropirate matrix (hot encoded) response\n",
    "    y_train_m, y_test_m = [indicator_to_matrix(x,yelp_data.docs_label_index)  for x in (y_train[cv], y_test[cv])]\n",
    "    \n",
    "    out_sample_accuracies.append(accuracy(models[cv],x_test[cv],y_test_m))\n",
    "    in_sample_accuracies.append(accuracy(models[cv],x_train[cv],y_train_m))\n",
    "    \n",
    "    confustion_matrices.append(confusion_matrix(y_test_vec,predict_classes(models[cv],x_test[cv])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48.333333333333336, 25.0]\n"
     ]
    }
   ],
   "source": [
    "print([np.mean(in_sample_accuracies), np.mean(out_sample_accuracies)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,5) (4,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-4f1280ad20e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconf_perc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfustion_matrices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfustion_matrices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#  there has to be a better way to do row wise division\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_perc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   2940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2941\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[1;32m-> 2942\u001b[1;33m                             out=out, **kwargs)\n\u001b[0m\u001b[0;32m   2943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         ret = um.true_divide(\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,5) (4,4) "
     ]
    }
   ],
   "source": [
    "conf_perc = np.mean(confustion_matrices,axis=0).T/np.sum(np.mean(confustion_matrices,axis=0),axis=1) \n",
    "#  there has to be a better way to do row wise division \n",
    "print(conf_perc.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxlen = yelp_data.maxlen\n",
    "hidden_dim = 128\n",
    "nb_classes = len(yelp_data.docs_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model specification\n",
    "sequence = Input(shape=yelp_data.X_doc_seq.shape[1:3], dtype='float32')\n",
    "forwards = LSTM(hidden_dim,dropout_W=0.1,dropout_U=0.1)(sequence)\n",
    "backwards = LSTM(hidden_dim,dropout_W=0.1,dropout_U=0.1,go_backwards=True)(sequence)\n",
    "merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "after_dp = Dropout(0.1)(merged)\n",
    "output = Dense(nb_classes, activation='softmax')(after_dp)\n",
    "model = Model(input=sequence, output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile('adam', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data split\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(yelp_data.X_doc_seq,yelp_data.Y_doc_seq,test_size=0.2)\n",
    "\n",
    "# create appropirate matrix (hot encoded) response\n",
    "y_train, y_test = [indicator_to_matrix(x,yelp_data.docs_label_index)  for x in (y_train, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 21s - loss: 1.2636 - val_loss: 1.5206\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 13s - loss: 1.2633 - val_loss: 1.6461\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 13s - loss: 1.2423 - val_loss: 1.5810\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 13s - loss: 1.2538 - val_loss: 1.6171\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 13s - loss: 1.2432 - val_loss: 1.5024\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 13s - loss: 1.2427 - val_loss: 1.5294\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 13s - loss: 1.1943 - val_loss: 1.6045\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 14s - loss: 1.1928 - val_loss: 1.5866\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 16s - loss: 1.1774 - val_loss: 1.5520\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 13s - loss: 1.1523 - val_loss: 1.5807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f875530dd10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model specs\n",
    "batch_size = 32\n",
    "num_epoch = 10\n",
    "# train model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=num_epoch,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 3 1]\n",
      " [1 0 0 5 1]\n",
      " [2 0 0 2 2]\n",
      " [4 0 1 7 0]\n",
      " [3 0 2 8 6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "y_pred_vec = [ pred.argmax() for idx, pred in enumerate(predictions)]\n",
    "y_test_vec = [ pred.argmax() for idx, pred in enumerate(y_test)]\n",
    "\n",
    "confustion_matrix = confusion_matrix(y_test_vec,y_pred_vec)\n",
    "print(confustion_matrix) # class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 9, 1: 5, 2: 5, 3: 16, 4: 15}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class_balance = {1: 0, 2:0, 3:0, 4:0, 5:0}\n",
    "class_balance_test = {}\n",
    "\n",
    "for i in y_test_vec:\n",
    "    if(i in class_balance_test): class_balance_test[i] += 1\n",
    "    else: class_balance_test[i] = 1\n",
    "class_balance_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('yelp_bi_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "jsonfile = open('yelp_bi_lstm.json', 'w')\n",
    "json.dump(model_as_json_string, jsonfile)\n",
    "jsonfile.write('\\\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('yelp_bi_lstm.json') as data_file:\n",
    "    model_as_json_string = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "current_model = model_from_json(model_as_json_string)\n",
    "current_model.load_weights('yelp_bi_lstm.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
